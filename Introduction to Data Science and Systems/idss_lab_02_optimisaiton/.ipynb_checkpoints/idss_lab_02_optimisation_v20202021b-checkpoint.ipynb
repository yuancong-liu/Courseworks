{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93debdb65d25ac12db35b7abd0b860da",
     "grade": false,
     "grade_id": "cell-9995d325f17152b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Introduction to Data Science and Systems 2020-2021<small><small>v20202021b</small></small>\n",
    "## Lab 2: Optimisation\n",
    "#### - ***you should submit this notebook on Moodle along with one pdf file (see the end of the notebook and Moodle for instructions)***\n",
    "---\n",
    "#### University of Glasgow, JHW (amended by BSJ and NP)\n",
    "\n",
    "\n",
    "$$\\newcommand{\\vec}[1]{ {\\bf #1}} \n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44659804d65174d6a7f44613176b3f8d",
     "grade": false,
     "grade_id": "cell-32f2e0771a12134c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Purpose of this lab\n",
    "\n",
    "This lab should help you:    \n",
    "* understand how optimisation can be used to solve approximation problems.\n",
    "* understand how learning can be seen as an optimisation problem.\n",
    "* use automatic differentiation to accelerate optimisation.\n",
    "\n",
    "You will implement a very simple form of **deep learning** in this lab, using first-order optimisation to learn an approximating function. This is just a function and you don't need to know anything about deep learning!\n",
    "\n",
    "We highly recommend starting watching this video [**How machines learn**](https://www.youtube.com/watch?v=IHZwWFHWa-w), which conveys excellent intuitions and visualisations about optimisaiton in the context of neural networks (in a similar style as the lecture material presents the concept).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "17da7293cc3717038609851f7b9c262d",
     "grade": false,
     "grade_id": "cell-c9d783c37cf38ce0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Guide\n",
    "\n",
    "Lab 2 is structured as follows (with two main task sections):\n",
    "\n",
    ">-    **Task A: Tour of optimizaiton (random search, hill-climbing and gradient descent from scratch)**\n",
    ">-    **Task B: Optimising the weights in a basic neural network for face pose estimation**\n",
    ">-    **Appendix: Marking Summary (and additional metadata)**\n",
    "\n",
    "We recommend you read through the lab *carefully* and work through the tasks.\n",
    "\n",
    "#### Material and resources \n",
    "- It is recommended to keep the lecture notes (from lecture 3 in particular) open while doing this lab exercise. \n",
    "    * ... and you should, of course, be prepared to access some of the recommended material.\n",
    "- If you are stuck, the following resources are very helpful:\n",
    " * [NumPy cheatsheet](https://github.com/juliangaal/python-cheat-sheet/blob/master/NumPy/NumPy.md)\n",
    " * [NumPy API reference](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    " * [NumPy user guide](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.html)\n",
    "\n",
    "#### Marking and Feedback\n",
    "This assessed lab is marked using two different techniques;\n",
    "\n",
    "- Autograded with feedback; you'll get immediate feedback.\n",
    "- Autograded without (immediate) feedback (there will typically be a small demo/test so you can be confident that the format of your answer is correct).\n",
    "\n",
    "*Note*: auto-graded results are always provisional and subject to change in case there are significant issues (this will usually be in favour of the student).\n",
    "\n",
    "#### Help \\& Assistance\n",
    "- This lab is graded and the lab assistants/lecturer can provide guidance, but we can (and will) not give you the final answer or confirm your result.\n",
    "\n",
    "#### Plagiarism\n",
    "- All submissions will be automatically compared against each other to make sure your submission represents an independent piece of work! We have provided a few checks to make sure that is indeed the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "786205cb4f6a218f85f3a7d0c0baad6c",
     "grade": false,
     "grade_id": "cell-9487c48910cec853",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "You need to install `autograd`. The cell below will autoinstall this for you if the machine you are using does not already have it installed.\n",
    "\n",
    "***Note***: It is often easier to install packages externally to the notebook, especially on macOS. You can find more information [here](https://github.com/HIPS/autograd) in case the below install instructions fails. Make sure you install the packages in the correct Python installation / virtual environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import autograd.numpy as np\n",
    "    from autograd import grad, elementwise_grad    \n",
    "    from autograd.misc.flatten import flatten\n",
    "    print(\"autograd succesfully imported. Everything OK so far. \")\n",
    "except:\n",
    "    import os\n",
    "    import sys     \n",
    "    print(sys.version)\n",
    "    print(os.path)\n",
    "    # couldn't import, install the package\n",
    "    print(\"autograd not found.\\nInstalling autograd from git...\")\n",
    "    !pip install --no-cache --force --user git+git://github.com/HIPS/autograd\n",
    "    print(\"Please restart the kernel (Kernel/Restart) and run the import cells again.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard imports\n",
    "# Make sure you run this cell!\n",
    "from __future__ import print_function, division\n",
    "#import numpy as np  # NumPy we use autograd's wrapped version !\n",
    "import os\n",
    "import sys\n",
    "import binascii\n",
    "from unittest.mock import patch\n",
    "from uuid import getnode as get_mac\n",
    "\n",
    "# custom utils\n",
    "from jhwutils.checkarr import array_hash, check_hash\n",
    "import jhwutils.image_audio as ia\n",
    "import jhwutils.tick as tick\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# reset the visble marks counter\n",
    "tick.reset_marks()\n",
    "\n",
    "# special hash funciton\n",
    "def case_crc(s, verbose=True):\n",
    "    h_crc = binascii.crc32(bytes(s.lower(), 'ascii'))\n",
    "    if verbose:\n",
    "        print(h_crc)\n",
    "    return h_crc\n",
    "\n",
    "# this command generaties a unique key for your system/computer/account\n",
    "uuid_simple = ((\"%s\") % get_mac())\n",
    "uuid_str = (\"%s\\n%s\\n%s\\n%s\\n%s\\n\") % (os.path,sys.path,sys.version,sys.version_info,get_mac())\n",
    "uuid_system = case_crc(uuid_str,verbose=False) \n",
    "\n",
    "# Set up Matplotlib\n",
    "import matplotlib as mpl   \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Everything imported OK\")\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a09a96dc7fb42fe8e57b680759fa905",
     "grade": true,
     "grade_id": "cell-bec61a938806415d",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-task**: provide your personal details in two variables:\n",
    "\n",
    "* `student_id` : a string containing your student id (e.g. \"1234567x\"), must be 8 chars long.\n",
    "* `student_typewritten_signature`: a string with your name (e.g. \"Adam Smith\") which serves as a declaration that this is your own work (read the declaration of originality when you submit on Moodle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93b8b8a601458aa22e7cd431ec75ebe3",
     "grade": false,
     "grade_id": "cell-771662bf2bc8fe6a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "student_id = \"\" # your 8 char student id\n",
    "student_typewritten_signature = \"\" # your full name, avoid spceical chars if possible\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35439711087bf0d237b1cd165016bc27",
     "grade": true,
     "grade_id": "cell-51de70c85bb54040",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## We will print your info to a pdf file at the end of the notebook - \n",
    "# including the Declaration of Originality - which must be uploaded alongside \n",
    "# the actual notebook. You should also see two green \"v [0 marks] \n",
    "# indicating that your info meet the basic std.\n",
    "\n",
    "with tick.marks(0): # you don't get any credit for remembering your student id. This is just a test!\n",
    "    assert(len(student_id)==8)\n",
    "\n",
    "with tick.marks(0):  # you don't get any credit for remembering your own name! This is just a test!\n",
    "    assert(len(student_typewritten_signature)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Task A: Objective functions and gradient descent\n",
    "\n",
    "### Background\n",
    "\n",
    "If you are unsure about objective functions, read [the supplement](idss_lab_topic_03_optimisation_supplement_objective_functions.ipynb) or the first part of the lecture notes from Lecture 4.\n",
    "\n",
    "* An optimisation *problem* has **parameters**, (possibly) **constraints** and an **objective function**.\n",
    "\n",
    "* An optimisation *algorithm* has **hyperparameters** which determine how the search for the best parameter setting is conducted (for example, how big of a step to take when trying to move down the gradient of a function).\n",
    "\n",
    "* We will be working with **differentiable** objective functions, where we can compute the gradient of the function at any point, and use this information to quickly move towards the minimum.\n",
    "\n",
    "### Simple optimisation\n",
    "\n",
    "Suppose we have a 2D parameter space ($\\theta \\in \\real^2$), and an objective function we want to minimise:\n",
    "\n",
    "$$L(\\theta) = \\sum\\limits_{i=0}^{I-1}(\\theta_i-i)^2$$\n",
    "\n",
    "where $I$ is the length of the vector $\\theta$ (i.e. $I=2$ for a 2D problem).\n",
    "\n",
    "We can write this in code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "356cba9113871823e918ba22ca0c29ce",
     "grade": false,
     "grade_id": "cell-59ea677988fed208",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Define our objective function. In this case we formulate it as a \n",
    "# loss function (to be minimised) that can take theta of any length (not only 2D).\n",
    "def l(theta):\n",
    "    i = np.arange(len(theta))   # 0, 1,...\n",
    "    return np.sum((theta-i)**2) # sum of squared difference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1871a13616bf791117b0fb7b23dc6dbd",
     "grade": false,
     "grade_id": "cell-2681be343e68a05e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# some random point\n",
    "theta_0 =  np.array([0.1, 0.2])\n",
    "\n",
    "# compute loss (objective function value) at this point (in 2D)\n",
    "print(\"The loss function at %s evaluates to %s\" % (theta_0,l(theta_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "122e2d177466c41a434de59540cbe1d4",
     "grade": false,
     "grade_id": "cell-4f68a1b967d196b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can visualize the loss function in 3D... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the objective function as a function of parameters\n",
    "space = np.linspace(-2,2,50)\n",
    "xm, ym = np.meshgrid(space, space)\n",
    "pts = np.stack([xm.ravel(), ym.ravel()]).T\n",
    "\n",
    "# compute the objective function at a grid\n",
    "# of different parameter values\n",
    "zm = np.zeros((pts.shape[0]))\n",
    "for i in range(pts.shape[0]):\n",
    "    zm[i] = l(pts[i])\n",
    "%matplotlib inline\n",
    "\n",
    "## plot in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig_a1a = plt.figure(figsize=(12,12))\n",
    "ax = fig_a1a.add_subplot(1,1,1, projection='3d')\n",
    "zm = np.array(zm).reshape(xm.shape)\n",
    "sf = ax.plot_surface(xm,ym,zm,cmap='viridis')\n",
    "ax.set_zlabel(\"$L(\\\\theta)$\")\n",
    "\n",
    "ax.set_xlabel(\"$\\\\theta_0$\")\n",
    "ax.set_ylabel(\"$\\\\theta_1$\")\n",
    "ax.set_title(\"Objective function as a function of the parameters\")              \n",
    "ax.view_init(25, 50) # try changing this to ispect teh funciton from different angles\n",
    "fig_a1a.colorbar(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de6b03a9f6686f1b7cdf40232692b298",
     "grade": false,
     "grade_id": "cell-f57fbd12791e380e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "**Task A.1 Optional warm-up - manual optimisation and visualisation [ungraded, no marks]**\n",
    "\n",
    "We encourage you to visualise the loss function as a contour plot and determine if the function is convex or not (hint see the lecture notes for relevant code).\n",
    "\n",
    "- The figure (`fig_a1b`) should contain a contour plot of $L(\\theta)$ or $log(L(\\theta))$ including contour lines and labels. It should contain a red marker at the minimum (i.e. you should solve the optimisation problem manually, which is almost trivial in this case and only require you to look at the contour plot). The figure should follow best practices in visualization, e.g. following [Ten Simple Rules for Better Figures](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1003833&type=printable).\n",
    "- `A1_EXPLANATION`: should contain a brief explanation if the function is convex (from both the images and formulation of the objective function) and argue how you can see that. \n",
    "\n",
    "***This task is ungraded but we encourage you to discuss your visualisation and explanation with your group members and lab assistant.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d417199b4286eab24c3d232201735f6",
     "grade": false,
     "grade_id": "cell-28f25dfafd640e66",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the contour plot\n",
    "# Do not modify the figure\n",
    "fig_a1b = plt.figure(figsize=(10,10)) # DO NOT MODIFY\n",
    "ax_explanation = fig_a1b.add_subplot(4,1,(3,4))  # DO NOT MODIFY\n",
    "ax = fig_a1b.add_subplot(4,1,(1,2))  # DO NOT MODIFY\n",
    "####################################\n",
    "\n",
    "A1_EXPLANATION = \"\"\"\n",
    "[YOUR ANSWER HERE]\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "###################################################\n",
    "### This adds your text (do not change this!)\n",
    "ax_explanation.axis('off')\n",
    "try:\n",
    "    ax_explanation.text(0,0,(\"ANSWER A1:\\n%s\" % A1_EXPLANATION), fontsize=12)\n",
    "except:\n",
    "    ax_explanation.text(0,0,\"ERROR A1_EXPLANATION NOT PROVIDED\") \n",
    "    raise NotImplemented() # this is to give you an option to define A1_EXPLANATION above! \n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97459036186f7044a84bdeb00ecab0a4",
     "grade": false,
     "grade_id": "cell-774530df3435ad76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Using a computer, how can we find the value where this objective function has a minimum -- the setting of $\\theta$ that minmises $L(\\theta)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search: guessing solutions\n",
    "One solution would be to use purely random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(l, max_iters, guess_fn,n_dim):\n",
    "    best_guess, best_loss = None, None    \n",
    "    for i in range(max_iters):\n",
    "        # random guess\n",
    "        guess = guess_fn(n_dim)\n",
    "        loss = l(guess) # work out how bad it is\n",
    "        # check if we beat the record\n",
    "        if best_loss is None or l(guess)<best_loss:\n",
    "            best_loss = l(guess)\n",
    "            best_guess = guess\n",
    "    return best_guess\n",
    "\n",
    "# guess a vector between -10 and 10, with n_dim elements \n",
    "def guess(n_dim):\n",
    "    return np.random.uniform(-30, 30, n_dim) \n",
    "\n",
    "# try 10 repetitions\n",
    "np.random.seed(2018)\n",
    "for i in range(10):\n",
    "    result =  random_search(l, 10000, guess, len(theta_0))\n",
    "    print(\"Best random guess {guess}, loss {loss:.4f}\".format(guess=result, loss=l(result)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* we might get close to the optimium but certainly not within the tolerance we would normally expect (e.g. loss< 1e-6)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "56247f82a8b25f272870f91be949c79e",
     "grade": false,
     "grade_id": "cell-5365270e7e024d21",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill climbing (or rather hill descent in our case)\n",
    "Random guessing does okay in this case as we are in 2D - but it will not work very well in high dimensions (you should try!), and only come somewhat close to `[0,1]`. But the space of the objective function is continuous, and our objective function might also be continuous. This means we could take advantage of the continuity; a small change in our parameters might lead to a small change in the objective function.\n",
    "\n",
    "**Task A.2** Write code to do hill-climbing search (keeing in mind that we want to minimise our function), instead of pure random search. Adjust the parameterisation of the search until the tests pass (this should be relatively easy). Note that the test will test against a **random** target; you can't hardcode the solution to just return a certain array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4a310dd312ade9212878f4d15041bec",
     "grade": false,
     "grade_id": "cell-2453ca37af003675",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# In this cell define three function\n",
    "#   hill_climbing(...): see description above\n",
    "#   guess(n_dim): takes number of dimensions as input and returns a random guess\n",
    "#   neighbour(x): takes a guess x as input and returns a modification of a current estimate for theta\n",
    "\n",
    "def hill_climbing(l, max_iters, guess_fn, neighbour_fn,n_dim):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def guess(n_dim):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def neighbour(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7413d88dd02d59f6dbe7f84401f535ea",
     "grade": false,
     "grade_id": "cell-0a607a3c587a4701",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Testing hill-climbing\n",
    "The code below will test your solution. It should be apparent that hill climbing does a much better job, getting to a good solution within 5000 iterations.\n",
    "\n",
    "You must have defined functions:\n",
    "    \n",
    "* `hill_climbing`\n",
    "* `guess`: returns a random guess (e.g. in [-30,30] for each dimension)\n",
    "* `neighbour`: returns a modification of a current estimate for theta\n",
    "\n",
    "How you implement and use these is up to you.\n",
    "\n",
    "The hill climbing optimiser should expect to optimise a 2 element parameter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68f891ec76e1347cd18bc5d1a673b4e2",
     "grade": true,
     "grade_id": "cell-70ba19798c551e5c",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Updated test for A2\n",
    "# try running hill_climbing 10 times with random targets\n",
    "with tick.marks(8):\n",
    "    np.random.seed(2019)\n",
    "    passed = True\n",
    "    n_dim = 2\n",
    "    for i in range(10):\n",
    "        target = np.random.uniform(0,4,n_dim)\n",
    "        print(\"Target %s\" % target)\n",
    "        # use a random target :)\n",
    "        def custom_l(theta):\n",
    "            return np.sum((theta-target)**2)          \n",
    "        # 5000 iterations\n",
    "        result = hill_climbing(custom_l, 5000, guess, neighbour,n_dim)\n",
    "        difference = custom_l(result)               \n",
    "        print(\"Loss on run {i} is {loss:.2e}\".format(i=i, loss=difference))        \n",
    "        if difference>1e-4:\n",
    "            passed = False\n",
    "    assert(passed)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond continuity: differentiability and gradient descent\n",
    "Having a continuous objective function made it easy to optimise this solution (look at the solutions found by hillclimbing compared to random search), compared to having to use pure random search. We can do much better, however.\n",
    "\n",
    "If we can differentiate `l` (the loss function) then we can use **gradient descent** to solve the problem, using the algorithm:\n",
    "\n",
    "$$\\vec{\\theta^{(i+1)}} = \\vec{\\theta^{(i)}} - \\delta \\nabla L(\\vec{\\theta^{(i)}}) \\,\\,\\,\\,\\,\\,\\,\\, [\\textit{Equation 1}]$$\n",
    "\n",
    "which only relies on:\n",
    "- the parameter vector: $\\vec{\\theta}$\n",
    "- the gradient vector (or Jacobian): $\\nabla L(\\vec{\\theta})$ \n",
    "- the step size: $\\delta > 0$ (keep in mind the Lipschitz constant when choosing a step size; see the lecture notes for details).\n",
    "- $i$ is the iteration number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "554bc8c406e2cfdcdf85161900d9cae9",
     "grade": false,
     "grade_id": "cell-b029dacc059f82b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Task A.3** \n",
    "\n",
    "By hand (using pen and paper) compute the gradient of $L(\\vec{\\theta})$, (i.e. containing $\\frac{\\partial l(\\vec{\\theta})}{{\\partial \\vec{x}_n}})$ elements), and specify the gradient vector based on your derivations. Implement it in a function called `grad_l_manual` that takes `theta` as input and returns the gradient vector $\\nabla L(\\vec{\\theta})$ .\n",
    "\n",
    "*Hint*: You may need to recall techniques from high-school and consult a resource on rules for computing the derivative (e.g. [Wikipedia](https://en.wikipedia.org/wiki/Derivative) or [Math is Fun](https://www.mathsisfun.com/calculus/)). The key lesson is how these gradients are combined to form the Jacobian.\n",
    "\n",
    "**Note:** You can leave this task for later as the subsequent tasks do not depend on it being correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "639f6415cbe092f2d7475dff6f3d5218",
     "grade": false,
     "grade_id": "cell-a0cc3880b0588fc9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_l_manual(theta):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return nabla_l # a (2,) vector with elements [dL(theta) / dtheta_0, dL(theta) / dtheta_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "976e15b0caacc9a6e8792f4a152d1586",
     "grade": true,
     "grade_id": "cell-1ed2bc6ce81ca63a",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograded test with immediate feedback [10 marks]\n",
    "#\n",
    "# Note: this test is a bit complicated and you do not have to understand it in detail\n",
    "#\n",
    "#\n",
    "\n",
    "n_dim = 2\n",
    "with tick.marks(10): \n",
    "    # Check that the manual_grad computes the correct values\n",
    "    np.random.seed(2019)        \n",
    "    true_hash = np.array([28.33172614495944,26.030790468858548,37.95132408224626,29.03968828492285,13.146474436485942,\n",
    "                          26.269026440517408,29.419569977726084,10.199851539629735,26.91897239707773, 23.573802161176502])  \n",
    "    mg_test=grad(l)(np.random.uniform(20,1,n_dim))        \n",
    "    assert(check_hash(mg_test, ((2,), 117.95242299297746)))    \n",
    "    \n",
    "    for i_test in range(10):       \n",
    "        target = np.random.uniform(0,4,n_dim) # do not change it changes the hash\n",
    "        # Manual\n",
    "        with patch('__main__.grad') as mock_grad: # put here to have some other hidden code in the loop ;-)        \n",
    "            with patch('__main__.elementwise_grad') as mock_elementwise_grad:                        \n",
    "                    m_grad = grad_l_manual(np.array(target))\n",
    "                    assert(mock_elementwise_grad.called==False) # \"You have used the autograd function - that's not ok!\")\n",
    "                    assert(mock_grad.called==False) # \"You have used the autograd function - that's not ok!\")                                                                                                           \n",
    "                    assert(check_hash(m_grad, ((2,),true_hash[i_test]))) # \"You have used the autograd function !\")                                                                                                                               \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "87f0019c361278922b15c51671bc7475",
     "grade": false,
     "grade_id": "cell-33cb53c941bbf47c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Deriving and implementing the gradient by hand can be tedious. `autograd` makes the whole process trivially easy, and can automatically compute the derivatives of our code without custom gradient vector implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3f1d22689d9c60639d5f3868495aece",
     "grade": false,
     "grade_id": "cell-91513d0c04e3a8f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## create a handle to a function which can compute the gradient of l with respect to theta\n",
    "# this is evaluated at a spcific point and does not return the analytic function you derived\n",
    "# above -- but that's fine for gradient descent.\n",
    "grad_l = grad(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be a vector, with as many dimensions as theta\n",
    "theta_0 = np.array([0.1, 0.2])\n",
    "\n",
    "print(\"theta_0: %s\" % theta_0)\n",
    "print(\"gradient vector of l(theta) evaluated at theta_0: %s\" % grad_l(theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to make sure that this also works for high dimension data (e.g. 10D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "23a46f829428364a00e1e1a5c7dd305f",
     "grade": false,
     "grade_id": "cell-6d5123c6b5ea4011",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The result is a **vector** which points in the direction where the objective function should be decreasing. We can try moving a small amount in this direction, computing the loss as we go.\n",
    "\n",
    "**Task A.4**\n",
    "Implement the gradient decent algorithm (as defined in Equation 1) which \n",
    "- iterates the update rule for 20 iterations (no other stopping rules)\n",
    "- uses a stepsize of 0.2. \n",
    "- uses the `grad_l` for gradient computation (not `grad_l_manual`).\n",
    "- starts in $\\theta^{(0)} = [-0.5, 2.75]$\n",
    "- print (in each iteration) the current solution and the loss of that solution\n",
    "- store the final solution in `theta_20`\n",
    "\n",
    "*Hint*: This should be a simple for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f88af8f9c55ac78f258a48c0067119a9",
     "grade": false,
     "grade_id": "cell-386da80c85cf4def",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_0 = np.array([-0.5, 2.75])\n",
    "delta = 0.2 # how big of steps to take\n",
    "n_steps = 20\n",
    "print(\"Initial loss: %s\" % (l(theta_0)))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# print your final solution (hopefully something close to the optimum)\n",
    "print(\"Solution=%s\" % theta_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ae916be04788c915a6350f75e421c1a",
     "grade": true,
     "grade_id": "cell-eaf1e7ec5afffdc1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Sanity check that the size of theta_20 is correct\n",
    "with tick.marks(0):\n",
    "    assert(check_hash(0*theta_20,( (2,) , 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d022a432e9fd12970f16d199e588517",
     "grade": true,
     "grade_id": "cell-dbc0e045ca9c53f1",
     "locked": true,
     "points": 12,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden, autograded test [12 marks] - you should be able to manually validate that the method is correct before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You hopefully noticed that gradient decent reduces the objective function value, and gets very close to the true solution. While hill-climbing took thousands of iterations, and random search never got close to a good solution, this approach finds a very precise solution in only 15 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: The face direction  problem\n",
    "Given an image of a head, like the rendering of the statue below, can we predict how it is oriented in space, *directly from the pixels*? For example, the image below:\n",
    "\n",
    "<img src=\"new_data/face_0000.png\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the same face, but a different orientation (or *pose*) to this one:\n",
    "\n",
    "<img src=\"new_data/face_0001.png\" width=30%>\n",
    "\n",
    "We can characterise the pose in terms of the:\n",
    "\n",
    "* *elevation* (head tilting with chin up/down) and\n",
    "* *azimuth* (head rotating left to right)\n",
    "\n",
    "The images above have similar elevation (the noses are roughly level in both pictures), but the azimuths are very different.\n",
    "\n",
    "***How can we use optimisation to solve this problem?*** We need to have a *parameterisable function* that somehow maps from images to poses, and then adjust the parameters until the function maps face images onto poses. That is, we want to find some function $f$ that takes an image as input, and outputs a face poses as output. $f$ must be configurable with a vector of parameters $\\theta$.\n",
    "\n",
    "* $\\vec{x}$ will be the face image, represented as a simple vector, by unravelling the image\n",
    "* $y$ will be the predicted face pose, as a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55fdd1bdb0bca866a5b75c9f1d61af6e",
     "grade": false,
     "grade_id": "cell-888bc7b20de5ce72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Approximation\n",
    "We will be trying to approximate a function. This means we have an objective function of the form:\n",
    "\n",
    "$$L(\\theta) = \\|f(\\vec{x};\\theta)-y\\|_2$$\n",
    "\n",
    "where we measure the difference between a predicted output $f(\\vec{x};\\theta)$ and a real, expected output $y$, and try and minimise that difference by choosing a good setting for $\\theta$.\n",
    "\n",
    "We will build a simple \"deep learning\" system. We will completely ignore many of the important problems in machine learning, like overfitting, regularisation, efficient network architectures and fair evaluation,  and concentrate on using first-order optimisation to find a function that approximates a known transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c5b5296f15d8ae96c5bc8f8db9e9dc7",
     "grade": false,
     "grade_id": "cell-97248ff7fe694f78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Loading the data\n",
    "There are 698 images of a face in different poses in the file `new_data/face_strip.png`. The faces are 64x64 grayscale images, and\n",
    "have been stacked into one very long strip.\n",
    "    \n",
    "You can load an image with `ia.load_image_gray()`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "045185c8c1b280ecb8f3338b0d1ae45b",
     "grade": false,
     "grade_id": "cell-470fbde7f2590726",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load a single example image\n",
    "# *must* divide by 65535.0 to normalise the result\n",
    "face_img = ia.load_image_gray('new_data/face_0000.png') / 65535.0\n",
    "print(face_img.shape)\n",
    "ia.show_image_mpl(face_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the data\n",
    "\n",
    "**Task B.1**\n",
    "\n",
    "* Load the face data from `face_strip.png` into a single tensor `faces`.\n",
    "* Reshape/rearrange it to a 698x64x64 tensor, and then slice it to take *every second* row and column of each image, giving a 698x32x32 tensor `faces`.\n",
    "* Normalize it to 0.0-1.0, as above.\n",
    "* Use `np.loadtxt` to load the file `new_data/face_pose_degrees.txt` as an 2x698 and transpose it into a 698x2 array `face_poses`. This contains the known orientations of the faces, *in the same order as the faces on the strip*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8d08fde797c8695b3f2c33b8cb4c461c",
     "grade": false,
     "grade_id": "cell-ae0706665b4f0dd4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "683eb50707bae8033cb8a3c375ec3771",
     "grade": false,
     "grade_id": "cell-b102d31a68117c49",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you did this correctly, you'll see a low-resolution face animation when you run the code below. The animation will jump around apparently random poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d94180b7f241cafda849f3348363bec",
     "grade": false,
     "grade_id": "cell-66a7cceb1b9383aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ia.show_gif(faces[0:10,:,:], width=\"150px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "888693118766603bdb174e51cfeba4a5",
     "grade": true,
     "grade_id": "cell-74109f823661f6c2",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with tick.marks(6):\n",
    "    print(array_hash(faces))\n",
    "    assert(check_hash(faces, ((698, 32, 32), 87092746113.97925)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b643bbed9392ceddf5a19d41499bd22c",
     "grade": true,
     "grade_id": "cell-9a8774e54e147361",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    print(array_hash(face_poses))\n",
    "    assert(check_hash(face_poses, ((698, 2), -1401906.2345336243)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7cf380dc7292fb5f91398eda390d3679",
     "grade": false,
     "grade_id": "cell-bd3e1e41c5a78ee8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Image vectors\n",
    "\n",
    "We have to be able to write this problem in the form:\n",
    "\n",
    "$$L(\\theta) = \\|f(\\vec{x};\\theta)-\\vec{y}\\|_2$$\n",
    "\n",
    "Every input vector $\\vec{x}$ must have a corresponding matched expected output $\\vec{y}$, and we need a function $f$ that depends on $\\vec{x}$ and $\\theta$.\n",
    "\n",
    "### Inputs\n",
    "What is $\\vec{x}$? How can we define the input to this function? We need to have one vector per example; that is a matrix with one row per face image. We can do this by reshaping the `faces` tensor to unravel the 32x32 pixel image into a single 1024 dimensional vector.\n",
    "\n",
    "Each image is then represented a single point in a 1024 dimensional space; the space of all 32x32 grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f973e03bedd63191c47748ff6a320db",
     "grade": false,
     "grade_id": "cell-89ef28430ccb91ad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Task B.2**\n",
    "Reshape the face tensor to a 698*1024 matrix; each row being a face image as a single \"unravelled\" vector. Store this in `face_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07b145ed8293e0ccb1a1f7c7a8eea0eb",
     "grade": false,
     "grade_id": "cell-c67148193040371a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6cb0031a6af24abf763d586311bc46d6",
     "grade": true,
     "grade_id": "cell-cc1e3b5a7eb0a206",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    print(array_hash(face_inputs))\n",
    "    assert(check_hash(face_inputs,((698, 1024), 87092746113.97925)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8058ebd96724815a92ac9b0eb50b7a70",
     "grade": false,
     "grade_id": "cell-13a804f744c68aae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### The expected outputs\n",
    "$\\vec{y}$ will be the *known* face orientation for each image. This is the data in `face_poses`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f5b7c81677e78f5d0990145b1ae3800",
     "grade": false,
     "grade_id": "cell-76736be6d19b9cb2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The neural network we will define will map all predictions to the range [-1,1], so $\\vec{y}$ must be scaled to be in this range. The face orientations are currently stored in *degrees*.\n",
    "\n",
    "**Task B.3**\n",
    "\n",
    "Rescale `face_poses` by dividing it by 180.0 and storing the result in `expected_face_orientations`. This will reduce the maximum and minimum values to well inside the range [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4db9da6947de73770eabca928b7c9f5",
     "grade": false,
     "grade_id": "cell-08c67ced4ba3ff66",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acb140fcefbe6647bb6397c8e948495b",
     "grade": true,
     "grade_id": "cell-8cc3aa488fbc83cd",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Min pose value:{min}, max pose value:{max}\".format(min=np.min(expected_face_orientations), max=np.max(expected_face_orientations)))\n",
    "\n",
    "with tick.marks(6):\n",
    "    print(array_hash(expected_face_orientations))\n",
    "    assert(check_hash(expected_face_orientations, ((698, 2), -7788.3679696312465)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "179451223e71f3350ce2983c343832d7",
     "grade": false,
     "grade_id": "cell-6f3c28b34f2a0cfe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you have done this correctly, you will see the faces laid out below, where the position the face is plotted corresponds to the orientation of the face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(face_inputs.shape)\n",
    "print(expected_face_orientations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3760f1b6d628a13fa0285f9d4959c22",
     "grade": false,
     "grade_id": "cell-4adeb87a42627eb9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def show_faces(images, positions):\n",
    "    # show the faces, and their orientations\n",
    "    fig = plt.figure(dpi=200)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_facecolor('k')\n",
    "    sz = 0.025\n",
    "    for i in range(0,len(positions),4):\n",
    "        pos = positions[i]\n",
    "        \n",
    "        # show the image at the output position\n",
    "        ax.imshow(images[i].reshape(faces.shape[1],faces.shape[2]), \n",
    "                  extent=[pos[0]-sz, pos[0]+sz, -pos[1]*8-sz, -pos[1]*8+sz],\n",
    "                  vmin=0, vmax=1, cmap='gray')\n",
    "        \n",
    "    ax.axhline(0, lw=1)\n",
    "    ax.axvline(0, lw=1)\n",
    "    ax.text(0,0.55, '+Elevation', color='w', fontdict={\"fontsize\":6})\n",
    "    ax.text(0,-0.55, '-Elevation', color='w', fontdict={\"fontsize\":6})\n",
    "    ax.text(0.42,0.0, '+Azimuth', color='w', fontdict={\"fontsize\":6})\n",
    "    ax.text(-0.55,0.0, '-Azimuth', color='w', fontdict={\"fontsize\":6})\n",
    "    ax.set_xlim(-0.6,0.6)\n",
    "    ax.set_ylim(-0.6,0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"Face orientations\")\n",
    "    \n",
    "# apply to the *known* inputs and outputs\n",
    "show_faces(face_inputs, expected_face_orientations)\n",
    "plt.gca().set_title(\"True face orientations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3beb07efb1b7885a0d5208e39a1f3d11",
     "grade": false,
     "grade_id": "cell-aa6769237e2d17c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## A deep network\n",
    "How will this parameterisable function that maps from images to orientations be defined? We will use a very specific and simple deep \"neural network\" predictor. This is an incredibly simple algorithm. It takes a vector, then repeatedly:\n",
    "\n",
    "* adds a small constant\n",
    "* takes the `tanh()` of the resulting vector; this simply squashes all the elements of the vector so they lie in the range [-1,1]\n",
    "* multiplies the vector by a (different) matrix\n",
    "\n",
    "We have to define the *shape* of each of the matrices which will be used to transform the vector, but we *optimise* to find the elements that go into those matrices. This is the \"learning\" part.\n",
    "\n",
    "Each of these steps is traditionally called a \"layer\" of the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33a511c7544c89052e157fc6dd80ca39",
     "grade": false,
     "grade_id": "cell-a4639a84a81a5d1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# a very basic neural network\n",
    "# the only slightly subtle thing is the unflattening, which is explained below\n",
    "def predict(x, theta, unflatten):        \n",
    "    for w in unflatten(theta):          \n",
    "        x = w.T @ np.tanh(x + 0.1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dabf10dd323d26416eb6da60a972bb5f",
     "grade": false,
     "grade_id": "cell-1fd6f86b63e3728c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Flattening and unflattening\n",
    "**READ THIS CAREFULLY**\n",
    "\n",
    "To be able to optimise this prediction function in the standard form, we have to package *all* of the things that could vary into a single \"flat\" parameter vector $\\theta$. `predict()` can unpack a list of matrices from a single vector if it is given the right `unflatten` parameter.\n",
    "\n",
    "We can use the `flatten` convenience function to make this easy.\n",
    "\n",
    "    theta, unflatten = flatten(list_of_matrices)\n",
    "\n",
    "takes a list of matrices `list_of_matrices` and returns them packed into a single 1D NumPy vector `theta` along with `unflatten`, a function which will reverse that process and unpack all the matrices when applied to `theta`.\n",
    "\n",
    "`flatten` is like a \"super-ravel\" which can be reversed by the `unflatten` function which it returns. See the examples below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e9668aacd2123314c9ddcb55dd89b56",
     "grade": false,
     "grade_id": "cell-83c9de17a30d47a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# create a list of matrices\n",
    "list_of_matrices = [np.zeros((3,3)), np.ones((2,4)), np.full((1,4), 2.0)]\n",
    "print(list_of_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ee1d2441206886426b9c591226e3545",
     "grade": false,
     "grade_id": "cell-ef5e7eb7b2906f6c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# convert to a flat vector\n",
    "# along with a *function* which will later unflatten theta back into a list of matrices\n",
    "theta, unflatten = flatten(list_of_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd2fbcc78345a9ddd6eebf54b86f03cd",
     "grade": false,
     "grade_id": "cell-2c0db5f2d783c089",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# the flattened version; a single vector\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1bd41d623a35288d65a83005febd2eac",
     "grade": false,
     "grade_id": "cell-1631573220781097",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# restore the flattened version to original shapes\n",
    "print(unflatten(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "82b27765a83e6708f753ddb1894433b5",
     "grade": false,
     "grade_id": "cell-aa4bf145f9aaa8f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Objective function\n",
    "\n",
    "**Task B.4**\n",
    "\n",
    "Write an objective function that will compare the predicted output to the expected output, for one set of input and output vectors, given `theta`, `unflatten`, `x` and `y`. Use the $L_2$ norm. The function should be of the form\n",
    "\n",
    "    def face_loss(theta, unflatten, x, y):\n",
    "        ...\n",
    "        return l # a scalar\n",
    "        \n",
    "The loss function will need to call `predict` to calculate `y_prime`, the predicted output to compare with `y`. Assume `x` and `y` are vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5e62f6e92503b073f729ac6b5add283d",
     "grade": false,
     "grade_id": "cell-1f3b30df6d228d1e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def face_loss(theta, unflatten, x, y):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8074985be5ed0a3db6692a26adc4f3f",
     "grade": true,
     "grade_id": "cell-4cfe774e9fdb84c5",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_network = [np.array([[1,0.5,-0.5], [0.0, 2.0, -1.0]]).T, \n",
    "                np.array([[2.0, 1.0], [1.0, -1.0]]).T]\n",
    "test_theta, test_unflatten = flatten(test_network)\n",
    "      \n",
    "with tick.marks(5):\n",
    "    assert(abs(face_loss(test_theta, test_unflatten, np.array([1,2,3]), np.array([-1, 1]))-3.37)<0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "The choice of the matrix shapes we use to do the prediction affects how well we will be able to model the transformation.\n",
    "\n",
    "In our example, we know we have 1024 dimensional inputs (32x32 face images unraveled into flat vectors) and 2 dimensional outputs (the pose vectors). So the we must start with 1024 dimensional vectors and end up with 2 dimensional vectors. \n",
    "\n",
    "However, we can introduce any number of intermediate matrices into the prediction function. This gives more parameters that could be tweaked, and more flexibility in how the mapping is learned; it makes the prediction function more *flexible*. If we have more intermediate matrices, we can \"warp space\" more vigorously.\n",
    "\n",
    "A very simple model might have a single 1024,2 matrix; this would be a simple linear transformation of the inputs.\n",
    "\n",
    "A more complex model might have a mapping with four matrices in sequence:\n",
    "\n",
    "    W[0]     W[1]      W[2]   W[3]\n",
    "    1024,32 -> 32,16 -> 16,8 -> 8,2\n",
    "\n",
    "This \"architecture\" maps the 1024 dimensional input vector to some 32 dimensional space, then to some 16 dimensional space, then to some 8 dimensional space, then to the 2 dimensional output. Every matrix has to have an output dimension which matches the input dimension of the following matrix. The matrices W[0], W[1], W[2], W[3]  specify how the vector at each layer gets mapped to the next layer.\n",
    "\n",
    "These sizes of these matrices are pretty much arbitrarily chosen here. They could have been 1024->50->20->2 or 1024->10->10->10->10->10->15->2 or many other variations. These all represent different kind of functions that can be learned, but this the specific choice turns out not to be critical. More matrices with more elements means a more flexible function which can learn more complicated things; but will be harder to optimise efficiently.\n",
    "\n",
    "We *don't know* what values should go into these matrices; they specify some unknown transformation of the vectors in each step. We have to optimise to find the elements of these matrices. We will *discover* a warping of space from the space of images onto the space of poses. For this lab, assume that the matrix shapes to be used are:\n",
    "\n",
    "    1024,32  32,16  16,8  8,2\n",
    "\n",
    "This will work well, without being too hard to optimise. You can alter this if you want, but the above version is known to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initialisation\n",
    "\n",
    "We need to set up some *initial conditions* for the optimisation process. We can define the shape of each matrix, but we don't know what the values of the elements in the matrices should be. So we just make a random guess.\n",
    "\n",
    "* We will create a function that generates initial conditions for the prediction function. \n",
    "* This function takes a list of matrix *shapes*, and create a corresponding list of randomly filled matrices.  \n",
    "* Each matrix generated will have the specified shape.  \n",
    "* The function takes a parameter `sigma` that specifies the spread of the random values chosen. We use `np.random.normal(0, sigma, shape)` to generate the random numbers.\n",
    "\n",
    "It returns the **flattened** version of the matrix list, along with the corresponding `unflatten` function.\n",
    "\n",
    "For example `initial_conditions([[2,3], [3,6]], 0.1)` returns the flattened version of a `(2,3)` and `(3,6)` shape random matrices.\n",
    "\n",
    "NB! Make sure you understand this function and what it returns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acf9b0777f8f0fed22981dcd8d1bb58d",
     "grade": false,
     "grade_id": "cell-7ae1fb24df5a9177",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initial_conditions(shape_list, sigma):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28bd2235f10b4aae77212d4c0f8efa61",
     "grade": true,
     "grade_id": "cell-ec776112fdaf80f6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Check that initial_conditions is correct\n",
    "test_theta, test_unflatten = initial_conditions([[8,4], [4,8], [2,4]], 0.1)\n",
    "matrices = test_unflatten(test_theta)\n",
    "with tick.marks(1):\n",
    "    assert(matrices[0].shape==(8,4))\n",
    "    assert(matrices[1].shape==(4,8))\n",
    "    assert(matrices[2].shape==(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random predictions\n",
    "We can use predict to see the effect of applying this function to the face images. Since all of the matrices are random, the result will be a random mess, where the positions plotted have no relation to the true orientations of the faces.\n",
    "\n",
    "Hint: You may need to run several times to get \"faces\" in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f406403944e90014e354c77638ab8621",
     "grade": false,
     "grade_id": "cell-e124ef00f019efdd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## create some test initial conditions\n",
    "# you may have to run a few times to get images in the image!\n",
    "\n",
    "random_theta, unflatten = initial_conditions( [[1024,32], [32, 16],  [16, 8], [8,2]], 0.2)\n",
    "# predict the outputs (will be random junk)\n",
    "predicted_outputs = [predict(face_inputs[i], random_theta, unflatten) for i in range(698)]\n",
    "show_faces(face_inputs, np.array(predicted_outputs))\n",
    "plt.gca().set_title(\"Random face orientations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task B.5** Write a function `total_face_loss` that computes the sum of the objective function value for *every* matched input and output pair from `face_inputs` and `expected_face_orientations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fcaf736b4e0eb1ac3a873bcd91c69a86",
     "grade": false,
     "grade_id": "cell-071b921e6287e3ee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# compute the sum of losses\n",
    "# for every pair of xs and ys\n",
    "def total_face_loss(theta, unflatten):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "34812ceeceb9af10a46496f3c82e948f",
     "grade": true,
     "grade_id": "cell-d8ddeb5754c4f7b1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "# create some test initial conditions\n",
    "random_theta, random_unflatten = flatten([np.random.normal(0,1,(1024,4)), np.random.normal(0,1,(4,2))])\n",
    "with tick.marks(2):\n",
    "    assert(np.allclose(total_face_loss(random_theta, random_unflatten), 1552.8832, atol=1e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent learning\n",
    "\n",
    "We could in principle use random search or hillclimbing but it will not give a very good results (trust me) and we jump directly to what we know is a beter method, namely gradient descent.\n",
    "\n",
    "In this task, you will have to define a function `sgd_learn`. This will perform a simple form of (stochastic) gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task B.6** Using automatic differentiation provided by `autograd`, compute the derivative of this loss function, and call it `grad_face_loss`. Hint: this is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c17fbb8cb76fced55e2e2a367b4ed756",
     "grade": false,
     "grade_id": "cell-21fd8352ecd3f855",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d500574a8d040e062707e8f1a98b764",
     "grade": true,
     "grade_id": "cell-bf35751a29bdce1c",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "flat, unflatten = flatten(np.array([1,2,3]))\n",
    "test_network = [np.array([[1,0.5,-0.5], [0.0, 2.0, -1.0]]).T, \n",
    "                np.array([[2.0, 1.0], [1.0, -1.0]]).T]\n",
    "test_theta, test_unflatten = flatten(test_network)\n",
    "\n",
    "with tick.marks(4):\n",
    "    assert(abs(np.sum(grad_face_loss(test_theta, test_unflatten, np.array([1,2,3]), np.array([-1, 1])))-4.479)<0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task B.7** \n",
    "\n",
    "Now we must define a function to perform stocahstic gradient descent.\n",
    "\n",
    "`sgd_learn` should:\n",
    "\n",
    "* take a list of matrix `shapes`, a `sigma` to specify the random initialisation of those matrices, a `step` size and a number of `iterations`\n",
    "* generate an initial `theta` from that set of shapes, using the `initial_condition()` function you defined above.\n",
    "* for each of the given number of iterations\n",
    "    * randomly select *ONE* input vector (from `inputs`) and matching output vector (from `outputs`).\n",
    "    * compute the gradient of the objective function for that image/output pair\n",
    "    * make a step, adjusting `theta` in the direction of this gradient, scaled by the step size\n",
    "    * print the iteration count every 500 iterations (i.e. for each 500 random data points) so you can see that the function is running correctly\n",
    "    * evaluate the objective function at each iteration, and print out the sum of the objective function value over the last 500 iterations in your print statement; append this value to an array called `hist_loss_500`\n",
    "    \n",
    "* return the flattened vector, the corresponding unflatten function and the loss in each iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function definition should look like:\n",
    "\n",
    "        def sgd_learn(shapes, inputs, outputs, sigma, step, iters):\n",
    "            ...        \n",
    "            return theta, unflatten, hist_loss_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You don't need to implement *any* sophistications like momentum or random restart. You don't need to collect the data into minibatches. The algorithm you implement should be very simple. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "10dcfe2af4dc155be24dda9e2f0cb29c",
     "grade": false,
     "grade_id": "cell-cfe8202b8cc20236",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sgd_learn(shapes, inputs, outputs, sigma=0.1, step=0.1, iters=10000):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return w, unflatten, hist_loss_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a825a04a2a43d70db28b90123166c78",
     "grade": true,
     "grade_id": "cell-c3b2ef10d4195405",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# verify that the shapes come out right\n",
    "with tick.marks(4):\n",
    "    test_theta, test_unflatten, hist_loss_500 = sgd_learn([[1024,2]], face_inputs, expected_face_orientations, 0.1, 0.1, 1)\n",
    "    unflattened = test_unflatten(test_theta)\n",
    "    assert(len(unflattened)==1)\n",
    "    assert((unflattened[0].shape)==(1024,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "01c1c6ffc6668b55228d23f2d4d79954",
     "grade": true,
     "grade_id": "cell-f77f00b8d245dcca",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## verify that some learning happens\n",
    "with tick.marks(8):\n",
    "    test_theta, test_unflatten, test_hist_loss_500 = sgd_learn([[1024,2]], face_inputs, expected_face_orientations, 0.01, 0.001, 0)\n",
    "    before_losses = [face_loss(test_theta, test_unflatten, x,y) for x,y in zip(face_inputs, expected_face_orientations)]\n",
    "    test_theta, test_unflatten, test_hist_loss_500 = sgd_learn([[1024,2]], face_inputs, expected_face_orientations, 0.02, 0.001, 200)\n",
    "    after_losses = [face_loss(test_theta, test_unflatten, x,y) for x,y in zip(face_inputs, expected_face_orientations)]\n",
    "    print(\"Mean loss before optimising %.2f; after optimising %.2f\" % (np.mean(before_losses),  np.mean(after_losses)))\n",
    "    assert(np.mean(after_losses)-np.mean(before_losses)<-0.01)\n",
    "    print(\"Something was learned!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task B.8**\n",
    "\n",
    "Use this function to learn an approximate mapping from face images to 2D vectors. You will have to choose:\n",
    "* a **step size** (values in the range 0.1 to 0.0001 are reasonable)\n",
    "* the **sigma** for the initial conditions (values in the range 0.5 to 0.005 are reasonable).\n",
    "\n",
    "These are **hyperparameters** of the optimisation process. \n",
    "\n",
    "* You should use *no more* than 20000 iterations in the learning process. \n",
    "\n",
    "**Warning: if your call to `sgd_learn` takes more than ten minutes to run, the autograder will not accept your result!**\n",
    "\n",
    "Use `[[1024,32], [32, 16],  [16, 8], [8,2]]` as the list of shapes, *or* choose your own set of matrix shapes (just don't make them so large the optimisation takes forever).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "53c2aafdc4ff02abbabcaea20bd9d359",
     "grade": false,
     "grade_id": "cell-80ac1062c78b9d10",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Run sgd_learn(...) in this cell\n",
    "## produce the output theta, unflatten\n",
    "## theta, unflatten = learn(...\n",
    "## LEAVE THE SEED HERE TO FORCE CONSISTENT RESULTS!\n",
    "np.random.seed(2019) # DO NOT MODIFY \n",
    "#############################################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35b1acae8c3e801e4190df0c06b9d28d",
     "grade": false,
     "grade_id": "cell-7cf84efe80519223",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# if your code worked, then it should be able to predict\n",
    "# what the orientation of the face is, from the image of the face\n",
    "# this should be very close to the faces image plotted above\n",
    "predicted_outputs = [predict(face_inputs[i], theta, unflatten) for i in range(698)]\n",
    "show_faces(face_inputs, np.array(predicted_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task B.9 Optional visualisation [ungraded, no marks]**\n",
    "\n",
    "We encourage you to produce a figure showing the convergence of your algorithm (`fig_b9`). It should of course follow best pratice for visualisation (see resources on Moodle if your are in doubt about this). You can use the previously implemented `hist_loss_500` array for this. Provide an description and analysis of the convergence behaviour based on the averaged loss function in `hist_loss_500` (max 200 words). \n",
    " \n",
    "***The task is ungraded but we recommend you discuss the solution with your lab group and lab assistant.***\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "74a884656ab6d5605131066de513daec",
     "grade": false,
     "grade_id": "cell-a8f6e878a1c8431a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the figure (do not chnage the fig handle name)\n",
    "fig_b9 = plt.figure(figsize=(15,15))\n",
    "ax_explanation = fig_b9.add_subplot(4,1,(3,4))\n",
    "ax = fig_b9.add_subplot(4,1,(1,2))\n",
    "#########################################\n",
    "\n",
    "B9_EXPLANATION = \"\"\"\n",
    "[YOUR ANSWER HERE]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "B9_EXPLANATION = \"\"\"\n",
    "- looks surprisingly smooth (but remember it is averaged over 500 iterations)\n",
    "- this is generally good convergence behavior but we would need to look at the non averaged verison to get a full picture\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "### This adds your text (do not change this!)\n",
    "ax_explanation.axis('off')\n",
    "try:\n",
    "    ax_explanation.text(0,0,(\"ANSWER A1:\\n%s\" % B9_EXPLANATION), fontsize=12)\n",
    "except:\n",
    "    ax_explanation.text(0,0,\"ERROR A1_EXPLANATION NOT PROVIDED\") \n",
    "    raise NotImplemented() # this is to give you an option to define A3_EXPLANATION above! \n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ff9c6b8bca31235b3a5978b806d136b",
     "grade": false,
     "grade_id": "cell-69e4e962e935bf01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Task B.10** \n",
    "\n",
    "In this part, we will automatically evaluate the performance of your model - you do not necessarily need to do anything to your code! Marks are awarded based on the quality of your model's performance and executed as hidden tests without immediate feedback.\n",
    "\n",
    "To evaluate the quality of your predictions we will use the following metric called the root mean squared error:\n",
    "\n",
    "$$rmse = \\sqrt {\\frac{1}{N\\cdot D}\\sum\\limits_{n,d}^{} {\\left( {{\\vec{y}_{n,d}} - \\vec{\\hat y}_{n,d}} \\right)^2} }$$ where $\\vec{y}$ is a vector with the true image orientation values (*elevation* and *azimuth*) for image $n$. $\\vec{\\hat y}=f(\\vec{x}_n;\\theta)$ is a vector with the predicted values for image $n$. $d$ is used to index the *elevation* and *azimuth* values. $N$ is the number of images and $D$ is the number of image orientation values (i.e. two).\n",
    "\n",
    "Your marks for this question will be awarded according to the following rule:\n",
    "> rmse < 0.50: +9 marks\n",
    ">\n",
    "> rmse < 0.25: +8 marks\n",
    ">\n",
    "> rmse < 0.10: +7 marks\n",
    ">\n",
    "> rmse < 0.03: +4 marks ; note only four marks -- we don't want to to iterate this for ever!\n",
    "\n",
    "So, you get more marks for answers that are better optimisers in the rmse sense.\n",
    "\n",
    "*Note*: you will have to implement the $rmse$ function yourself if you want to know your your final score before submitting it (this is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed879f4e84edb7066843f5cb5be34205",
     "grade": true,
     "grade_id": "cell-b716771933d3906c",
     "locked": true,
     "points": 9,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Hidden rmse test [9 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a847695aba6f998062407f70cbba2356",
     "grade": true,
     "grade_id": "cell-f6e84269dfdadce5",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Hidden rmse test [8 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1c8b2afc07340ef4b2568ffad150330",
     "grade": true,
     "grade_id": "cell-a711aa45240e87d2",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Hidden rmse test [7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f4ab7e756791495f3c62d376a28cc70",
     "grade": true,
     "grade_id": "cell-7225406ce8015c60",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Hidden rmse test [4 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "If you've got this far, you've managed to build a system which can predict the orientation of the face from a picture of the face. All that was needed was some simple gradient descent, which optimised a function parameterised with a 20656 dimensional vector (the number of elements in `theta`).\n",
    "\n",
    "This is not a perfect solution. It is not robust to noise, or to variations in the face position, scale or rotation. We have no idea if it generalises well to other images of the face than those we trained on. The mapping we used is extremely wasteful, and ignored the fact that pixels which are close together are probably related. All of these things would be fixed in a real deep learning approach, but the the principle remains the same. We create a parameterisable function, then optimise its parameters to align the approximation with some known training examples.\n",
    "\n",
    "# Extra material\n",
    "If you really want to understand what is going on in learning in deep networks, read [colah's blog](http://colah.github.io/posts/2015-08-Backprop/),  another [colah article](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/), and [Nielsen's free book](http://neuralnetworksanddeeplearning.com/), or [Deep learning from scratch](http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/). You don't need to know **any** of this for this lab though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "674705bad9b0399648f59efafceff82a",
     "grade": false,
     "grade_id": "cell-b8ceff22c3c2b533",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "819034820d13eb57a270260b4099c1f5",
     "grade": false,
     "grade_id": "cell-d08d4a75c8006022",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Submission on Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "452043f16b9f1542132881a3654a7bee",
     "grade": false,
     "grade_id": "cell-6c7b3819abd32986",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "We will generate the **one** pdf file you'll need to submit along with the notebook:\n",
    "\n",
    "*Note*: you do not need to worry about the formatting etc (that's predetermined); just make sure all your explanations are readable in the pdf and your'll be fine!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1a7a10cd62e69f304e4e201a8e893d1",
     "grade": false,
     "grade_id": "cell-b36e056221228fa7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Report generation - YOU MUST YOU RUN THIS CELL !\n",
    "#\n",
    "#\n",
    "# Ignore warnings regarding fonts\n",
    "#\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# File 1: declaration of originality with system info\n",
    "try:\n",
    "    f = open('uofg_declaration_of_originality.txt','r')\n",
    "    uofg_declaration_of_originality = f.read()\n",
    "except: \n",
    "    uofg_declaration_of_originality = \"uofg_declaration_of_originality not present in cwd\"\n",
    "\n",
    "try:\n",
    "    student_id.lower()\n",
    "except: \n",
    "    student_id=\"NORESPONSE\"\n",
    "try:\n",
    "    student_typewritten_signature.lower()\n",
    "except: \n",
    "    student_typewritten_signature=\"NORESPONSE\"\n",
    "\n",
    "fn = (\"idss_lab_02_optimisation_%s_declaration.pdf\" % (student_id.lower()))\n",
    "fig_dec = plt.figure(figsize=(10, 12)) \n",
    "fig_dec.text(0.1,0.1,(\"%s\\n\\n Student Id %s\\n\\n Typewritten signature: %s\\n\\n UUID System: %s\" % (uofg_declaration_of_originality,student_id, student_typewritten_signature, uuid_system)))\n",
    "   \n",
    "    \n",
    "# Combined: \n",
    "fn = (\"idss_lab_02_optimisation_%s_combined_v20202021b.pdf\" % (student_id))\n",
    "pp = PdfPages(fn)\n",
    "pp.savefig(fig_a1b)\n",
    "pp.savefig(fig_b9)\n",
    "pp.savefig(fig_dec)\n",
    "pp.close()\n",
    "\n",
    "with tick.marks(0):  # have you generated the combined file...? you don't actually get any credit for this just confirmation that the file has been generated\n",
    "    assert(os.path.exists(fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c49f393aa3efefce075e6b3af2fa46d2",
     "grade": false,
     "grade_id": "cell-c32d892a2f810ab0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**You must (for full or partial marks) submit via Moodle:**\n",
    "\n",
    "- this notebook (completed) after \"Restart and rerun all\":\n",
    "    - `idss_lab_02_optimisation_v20202021b.ipynb`\n",
    "    \n",
    "- the combined pdf (autognerated) containing the originality statement, optional figures and signature\n",
    "     - `idss_lab_02_optimisation_[YOUR STUDENT ID]_combined_v20202021b.pdf`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7436c9f8aa2f3b86c17ec7e36bc6b1a3",
     "grade": false,
     "grade_id": "cell-530afe415c5eb50f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b5d60dd0ad1fe8165cf318b6b31b3401",
     "grade": false,
     "grade_id": "cell-b0ccdfd6f69b03dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Appendix: Marking Summary (and other metadata)\n",
    "#### - make sure the notebook runs without errors (remove/resolve the `raise NotImplementedError()`) and \"Restart and Rerun All\" cells to get a correct indication of your marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3dd28c39f3bac8ccececa542ad64d393",
     "grade": true,
     "grade_id": "cell-1fa128795b30c9c3",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Marks total : \",\"100\")\n",
    "print(\"Marks visible (with immediate feedback): \",\"60\")\n",
    "print(\"Marks hidden (without immediate feedback): \",\"40\")\n",
    "print(\"\\nThe fraction below displays your performance on the autograded part of the lab that's visible with feedback (only valid after `Restart and Run all`:\")\n",
    "tick.summarise_marks() # \n",
    "print(\"- the autograded (and visible) marks account for at least 50% of the total lab assesment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
